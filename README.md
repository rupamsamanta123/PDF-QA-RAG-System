# ğŸ“– PDF Question-Answering App (RAG-based)

> A **PDF Question-Answering App** built with **RAG (Retrieval-Augmented Generation)**, allowing users to upload PDFs and ask context-based questions.  
> Powered by **Streamlit, LangChain, Ollama, and Chroma/FAISS** for efficient and accurate answers.  

[![Streamlit](https://img.shields.io/badge/Framework-Streamlit-FF4B4B?logo=streamlit&logoColor=white)](https://streamlit.io/)  
[![LangChain](https://img.shields.io/badge/Library-LangChain-3b82f6?logo=chainlink&logoColor=white)](https://www.langchain.com/)  
[![Ollama](https://img.shields.io/badge/LLM-Ollama-00A86B?logo=ollama&logoColor=white)](https://ollama.ai/)  
[![FAISS](https://img.shields.io/badge/Vector_DB-Chroma-orange?logo=databricks&logoColor=white)](https://github.com/facebookresearch/chroma)  
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)  

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ“Œ Project Overview  
This project is a **Retrieval-Augmented Generation (RAG) based Question-Answering System** where users can upload a **PDF document** and ask questions related to its content.  

It extracts the text, splits it into chunks, stores it in a vector database, and uses a **Large Language Model (LLM)** to provide accurate, context-aware answers.  
The RAG pipeline ensures that answers are **based on real document context** to minimize hallucination.  

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## âœ¨ Key Features  
- ğŸ“‚ **Upload PDF** and extract text automatically.  
- ğŸ” **Context-aware Q/A** using RAG (retrieves relevant chunks before answering).  
- âš¡ **Efficient Vector Search** with **Chroma/FAISS** backend.  
- ğŸ¤– **LLM-powered answers** using **Ollama + Llama 3.1**.  
- ğŸ–¥ï¸ **Interactive GUI** built with **Streamlit** for seamless use.  
- ğŸ”’ Handles temporary files securely during processing.  

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ› ï¸ Tools & Technologies  
- [![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?logo=streamlit&logoColor=white)](https://streamlit.io/) â€“ Interactive web interface  
- [![LangChain](https://img.shields.io/badge/LangChain-3b82f6?logo=chainlink&logoColor=white)](https://www.langchain.com/) â€“ RAG pipeline  
- [![Ollama](https://img.shields.io/badge/Ollama-00A86B?logo=ollama&logoColor=white)](https://ollama.ai/) â€“ Local LLM (`llama3.1`, `nomic-embed-text`)  
- [![FAISS](https://img.shields.io/badge/Vector_DB-Chroma/FAISS-orange?logo=databricks&logoColor=white)](https://github.com/facebookresearch/faiss) â€“ Vector similarity search  
- [![PyPDF](https://img.shields.io/badge/PDF-PyPDFLoader-blue?logo=adobeacrobatreader&logoColor=white)](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf) â€“ Extract text from PDFs  

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ“‹ Prerequisites  
Before running the project, make sure you have:  
- ğŸ Python **3.9+** installed  
- ğŸ“¦ **Ollama** installed and running locally â†’ [Install Ollama](https://ollama.ai/)  
- ğŸ“¥ Pull required models in Ollama:  
  ```bash
  ollama pull llama3.1
  ollama pull nomic-embed-text
  ```
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ“‚ Project Structure  

The repository is organized as follows:  

```bash
pdf-qa-rag/
â”‚â”€â”€ app.py              
â””â”€â”€ requirements.txt    
```

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸš€ Quick Start  

Follow these steps to set up and run the project locally:  

### 1ï¸âƒ£ Clone the Repository  
```bash
git clone https://github.com/muqadasejaz/pdf-qa-rag-system.git
```

2ï¸âƒ£ Create a Virtual Environment (Recommended)

```bash
# For Windows
python -m venv venv
venv\Scripts\activate
```

3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

4ï¸âƒ£ Install Ollama & Pull Required Models

- Download and install Ollama 

- Pull models locally:

```bash
ollama pull llama3.1
ollama pull nomic-embed-text
```

5ï¸âƒ£ Run the Application

```bash
streamlit run app.py
```

6ï¸âƒ£ Open in Browser

- Streamlit will provide a local URL (e.g. http://localhost:8501).
- Open it in your browser to access the PDF Question-Answering App.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ“± How to Use  

1. Start the app by running:  
   ```bash
   streamlit run app.py
   ```
2. Open the Streamlit link in your browser (default: http://localhost:8501).

3. ğŸ“‚ Upload a PDF

- Use the file uploader to select any PDF (e.g., notes, research papers, or reports).

4. âš¡ Processing

- The system will extract text, split it into chunks, and store them in the vector database.

5. ğŸ“ Ask Your Question

- Enter your query in the text box (e.g., "What is the main conclusion of this paper?").

6. ğŸ’¡ Get the Answer

- The LLM (Ollama + LangChain) retrieves the most relevant context and generates an accurate, context-based answer.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


## ğŸ—ï¸ Architecture  

The system follows a **RAG (Retrieval-Augmented Generation)** pipeline for answering questions from PDFs.  

<img width="1200" height="596" alt="architecture" src="https://github.com/user-attachments/assets/ecb02057-4c08-416f-9310-0192aa3138c5" />


### Workflow:
1. ğŸ“‚ **PDF Documents** â†’ Uploaded by the user.  
2. ğŸ” **LangChain Pipeline** â†’ Handles:  
   - PDF Text Extraction  
   - Text Chunking  
   - Vector Store (Chroma/FAISS)  
3. ğŸ¤– **Ollama LLM** â†’ Uses `llama3.1` for generating answers.  
4. ğŸ’¡ **Generated Response** â†’ Delivered back to the user.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ–¥ï¸ GUI Preview  

The application comes with a simple and interactive **Streamlit-based UI**.  

### ğŸ“Œ Home Screen  
Upload your PDF, enter your question, and get instant answers.  

<img width="1348" height="614" alt="home" src="https://github.com/user-attachments/assets/26ffe22c-98db-4fb4-b666-2a97f0096a2b" />


### ğŸ“Œ PDF Upload & Processing  
After uploading, the system extracts and processes the PDF text for querying.  

<img width="1061" height="389" alt="upload" src="https://github.com/user-attachments/assets/fcffe942-f5b4-4fac-a234-134dd8f17a23" />

### ğŸ“Œ Q&A Result  
Ask questions, and the system provides precise answers from the document.  

- Answer Query 1:

<img width="1290" height="630" alt="output1" src="https://github.com/user-attachments/assets/e956e0c1-067f-4ee0-819b-3cdda57648f7" />

- Answer Query 2:

<img width="1354" height="641" alt="query2" src="https://github.com/user-attachments/assets/59d7dd06-1723-4b8a-bb02-2d1cda5e6199" />


- Output:
  
<img width="1338" height="631" alt="query2_output" src="https://github.com/user-attachments/assets/ca995103-a4ef-469d-9356-9680b984cabf" />

- Wrong Query:

  <img width="1352" height="618" alt="wrong_query" src="https://github.com/user-attachments/assets/e2416423-b106-4681-9a73-9a51cdba3b18" />

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ™ Acknowledgements  

This project is built with the help of:  
- [LangChain](https://www.langchain.com/) â€“ for chaining components in the RAG pipeline  
- [Ollama](https://ollama.ai/) â€“ for running **LLaMA 3.1** locally  
- [Streamlit](https://streamlit.io/) â€“ for the user-friendly interface  
- [ChromaDB / FAISS](https://www.trychroma.com/) â€“ as vector databases for semantic search  
- [nomic-embed-text](https://huggingface.co/nomic-ai/nomic-embed-text) â€“ for text embeddings  

Special thanks to the **open-source community** for providing tools that make projects like this possible ğŸš€  

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


## ğŸ‘¤ Author

Muqadas Ejaz

BS Computer Science (AI Specialization)

AI/ML Engineer

Data Science & Gen AI Enthusiast

ğŸ“« Connect with me on [LinkedIn](https://www.linkedin.com/in/muqadasejaz/)  

ğŸŒ GitHub: [github.com/muqadasejaz](https://github.com/muqadasejaz)

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ“ License

This project is open-source and available under the [MIT License](LICENSE).

